# IronCoder Gesture Control for Claude Code

**Control Claude Code with hand gestures and voice!**

A macOS utility that combines AI-powered gesture recognition and voice transcription to control Claude Code through intuitive hand movements and speech.

## âœ¨ Features

- **ğŸ¤– AI-Powered Gestures**: Google Gemini Vision API for reliable gesture detection
- **ğŸ¤ Voice Input**: Push-to-talk with real-time Whisper transcription
- **ğŸ‘Š Dual-Hand System**: Left hand clutch + right hand gestures
- **ğŸ¨ Beautiful UI**: Modern card-based overlay with color-coded controls
- **âš¡ Zero Accidental Triggers**: Clutch mechanism prevents unintended activation
- **5 Core Gestures**: Voice, commit & push, clear input, start/stop server
- **ğŸ”§ Highly Configurable**: YAML-based configuration for all settings

## ğŸ® Gesture Commands

### Left Hand: Clutch Control
- **Closed Fist** â†’ Clutch **ENGAGED** (enables right-hand gestures)
- **Open/No gesture** â†’ Clutch **DISENGAGED** (all gestures ignored)

### Right Hand: Commands (only active when clutch engaged)

| Gesture | Action | Description |
|---------|--------|-------------|
| âœ‹ **Open Palm** | Voice Dictation | Push-to-talk: Hold to record, release to transcribe |
| âœŒï¸ **Peace Sign** | Start Dev Server | Types "start the dev server" + Enter |
| ğŸ‘ **Thumbs Up** | Commit & Push | Types "commit and push" + Enter |
| ğŸ‘ **Thumbs Down** | Clear Input | Sends Escape + Escape to clear input |
| â˜ï¸ **Pointing** | Stop Dev Server | Types "kill the running server" + Enter |

## ğŸ“¦ Installation

### Prerequisites
- macOS (tested on macOS 10.15+)
- Python 3.10+
- Webcam
- Google Gemini API Key ([Get one free](https://aistudio.google.com/app/apikey))

### Setup

1. **Clone the repository**:
   ```bash
   git clone <your-repo-url>
   cd gesture-control-claude
   ```

2. **Install dependencies**:
   ```bash
   pip3 install -r requirements.txt
   ```

3. **Configure Gemini API key**:
   Create a `.env.local` file:
   ```bash
   echo "GEMINI_API_KEY=your_api_key_here" > .env.local
   ```

4. **Grant camera permissions**:
   - System Settings â†’ Privacy & Security â†’ Camera
   - Allow Terminal/iTerm to access camera

5. **Grant accessibility permissions** (for keyboard simulation):
   - System Settings â†’ Privacy & Security â†’ Accessibility
   - Add Terminal/iTerm to the list

## ğŸš€ Usage

### Running the App

```bash
python3 main.py
```

### Quick Start Guide

1. **Launch** the application - webcam feed opens with overlay
2. **Position hands** in front of the camera
3. **Close left fist** to engage clutch (green border appears)
4. **Make gestures** with your right hand:
   - Hold **open palm** and speak, release when done
   - Show **thumbs up** to commit and push
   - Show **thumbs down** to clear input
   - And more!
5. **Press keys**:
   - `h` - Toggle hints overlay
   - `q` - Quit application

### Voice Input Tips

- **Hold open palm** gesture while speaking
- **Speak clearly** into your microphone
- **Release gesture** when finished speaking
- Text appears automatically in Claude Code input
- Uses Whisper AI for accurate transcription

## âš™ï¸ Configuration

Edit `config.yaml` to customize settings:

```yaml
clutch:
  hand: left                    # Which hand for clutch
  gesture: closed_fist
  require_stable_frames: 5      # Stability threshold

gestures:
  open_palm: voice_dictation
  peace_sign: start_dev_server
  thumbs_up: commit_push
  thumbs_down: clear_input
  pointing: stop_dev_server

settings:
  confidence_threshold: 0.7     # Hand detection confidence
  cooldown_ms: 500              # Delay between gestures
  require_terminal_focus: false # Terminal focus requirement
  camera_resolution: [640, 480]
  camera_fps: 20

gemini:
  model: gemini-2.5-flash       # Gemini model to use
  sample_interval: 0.5          # Seconds between API calls
  stability_frames: 2           # Consecutive detections needed
  resize_width: 512             # Image width sent to API
```

## ğŸ—ï¸ Architecture

```
Webcam Feed
    â†“
MediaPipe Hand Tracking (Clutch Detection)
    â†“
Google Gemini Vision API (Gesture Recognition)
    â†“
Action Handler
    â”œâ”€â†’ Whisper AI (Voice Transcription)
    â””â”€â†’ PyAutoGUI (Keyboard Simulation)
```

### Key Components

- **HandTracker**: MediaPipe wrapper for clutch detection
- **ClutchDetector**: Detects closed fist on left hand
- **GeminiGestureDetector**: AI-powered gesture recognition using Gemini Vision
- **AudioHandler**: Real-time audio recording and Whisper transcription
- **ActionHandler**: Executes commands and manages voice input
- **VisualFeedback**: Beautiful card-based UI overlay

### Technology Stack

- **[Google Gemini Vision API](https://ai.google.dev/)** - Gesture detection
- **[faster-whisper](https://github.com/SYSTRAN/faster-whisper)** - Speech-to-text
- **[MediaPipe](https://mediapipe.dev/)** - Hand tracking for clutch
- **[OpenCV](https://opencv.org/)** - Computer vision
- **[PyAutoGUI](https://pyautogui.readthedocs.io/)** - Keyboard automation

## ğŸ”’ Safety Features

1. **Clutch Mechanism**: Gestures only work when left fist is closed
2. **Frame Smoothing**: Requires stable gesture over multiple frames
3. **Cooldown Period**: Prevents rapid double-triggers
4. **Hand Loss Reset**: Resets state when hands leave frame
5. **API Rate Limiting**: Gemini calls throttled to 0.5s intervals

## ğŸ› Troubleshooting

### Gestures Not Triggering
- âœ… Ensure clutch (left fist) is engaged (green border)
- âœ… Make gestures clearly in front of camera
- âœ… Check camera permissions granted
- âœ… Verify `.env.local` has valid Gemini API key
- âœ… Try adjusting `confidence_threshold` in config

### Voice Input Not Working
- âœ… Hold open palm gesture while speaking
- âœ… Ensure microphone is working
- âœ… Check for "ğŸ™ï¸ Processing..." logs
- âœ… Verify faster-whisper installed: `pip3 show faster-whisper`
- âœ… Wait 3 seconds before releasing gesture (chunk processing)

### Gemini API Issues
- âœ… Check API key in `.env.local`
- âœ… Verify API quota: [Google AI Studio](https://aistudio.google.com/)
- âœ… Look for "ğŸ“¡ Sending frame to Gemini API..." in logs
- âœ… Check internet connection

### Poor Gesture Detection
- âœ… Improve lighting conditions
- âœ… Make gestures clearly and deliberately
- âœ… Reduce `gemini.sample_interval` for faster detection
- âœ… Decrease `gemini.stability_frames` for quicker response

### High CPU Usage
- âœ… Lower camera resolution in config
- âœ… Reduce camera FPS
- âœ… Increase `gemini.sample_interval` (fewer API calls)
- âœ… Close other applications

## ğŸ“ Project Structure

```
gesture-control-claude/
â”œâ”€â”€ main.py                         # Entry point
â”œâ”€â”€ config.yaml                     # Configuration
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ .env.local                      # API keys (create this)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ hand_tracker.py             # MediaPipe hand tracking
â”‚   â”œâ”€â”€ clutch_detector.py          # Left hand clutch detection
â”‚   â”œâ”€â”€ gemini_gesture_detector.py  # Gemini-powered gesture recognition
â”‚   â”œâ”€â”€ audio_handler.py            # Whisper audio transcription
â”‚   â”œâ”€â”€ action_handler.py           # Command execution
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ window_manager.py       # Window focus detection
â”‚       â””â”€â”€ visual_feedback.py      # Beautiful UI overlays
â””â”€â”€ README.md
```

## ğŸ”® Future Enhancements

- [ ] Custom gesture training via Gemini fine-tuning
- [ ] Gesture chaining for complex commands
- [ ] Right-hand clutch option for left-handed users
- [ ] Background service mode (menu bar app)
- [ ] Multi-language voice support
- [ ] Gesture macros/shortcuts
- [ ] Linux/Windows support

## ğŸ“ Development Notes

### Adding New Gestures

1. Add gesture description to `GeminiGestureDetector.GESTURES`
2. Add action method to `ActionHandler`
3. Map gesture to action in `action_handler.py`
4. Update `config.yaml` gesture mappings
5. Update visual feedback hints in `visual_feedback.py`

### Performance Optimization

- Camera: 640x480 @ 20fps (balances accuracy and performance)
- Gemini: 0.5s intervals with 512px images (manages API costs)
- Whisper: int8 quantization for faster inference
- MediaPipe: 0.7 confidence threshold (filters noise)

## ğŸ™ Credits

Built with:
- [Google Gemini Vision API](https://ai.google.dev/) - AI gesture detection
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) - Speech-to-text
- [MediaPipe](https://mediapipe.dev/) - Hand tracking
- [OpenCV](https://opencv.org/) - Computer vision
- [PyAutoGUI](https://pyautogui.readthedocs.io/) - Keyboard automation

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ’¬ Support

For issues, questions, or contributions, please open an issue on GitHub.

---

**Made with Claude Code** ğŸ¤–
